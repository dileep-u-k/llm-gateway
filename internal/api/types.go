// In file: internal/api/types.go

// Package api defines the public API contract for the LLM Gateway.
// These data structures are used for request binding and response serialization,
// serving as a stable, versioned interface for all client interactions.
package api


// Message defines the structure for a single message in a conversation history.
// This is part of the public API and is used in the GenerationRequest.
type Message struct {
	Role    string `json:"role"`
	Content string `json:"content"`
}

// GenerationRequest defines the structure for an incoming request to the /generate endpoint.
// It includes the user's prompt and all necessary configuration for the gateway to process it.
type GenerationRequest struct {
	// Prompt is the user's query or instruction.
	Prompt string `json:"prompt" binding:"required"`
	// UserID is an identifier for the end-user, crucial for logging, auditing, and rate-limiting.
	UserID string `json:"user_id"`
	// --- ADD THIS LINE ---
	// ConversationID links multiple requests together into a single chat session,
	// enabling features like model stickiness.
	ConversationID string `json:"conversation_id,omitempty"`
	// --- THIS FIELD IS NEW ---
	// History contains the list of previous messages in the conversation for context.
	History        []Message      `json:"history,omitempty"`
	// Config holds all the parameters that control how the gateway processes and routes the request.
	Config GenerationConfig `json:"config"`
}

// GenerationConfig holds all user-configurable parameters for a single LLM request.
type GenerationConfig struct {
	// Preference is the routing strategy the user prefers. The gateway's router will
	// use this to select the optimal model. Examples: "cost", "latency", "max_quality".
	Preference string `json:"preference,omitempty"`
	// --- ADD THIS LINE ---
	// ForceModel allows the user to bypass the router and pin a specific model to the
	// start of a new conversation.
	ForceModel string `json:"force_model,omitempty"`
	// MaxTokens sets the maximum number of tokens to generate in the response.
	MaxTokens int `json:"max_tokens,omitempty"`
	// Temperature controls the randomness of the output. Higher values (e.g., 0.8)
	// make the output more creative, while lower values (e.g., 0.2) make it more deterministic.
	// We use a pointer to distinguish between an unset value (nil) and a deliberate value of 0.
	Temperature *float32 `json:"temperature,omitempty"`
	// TopP is an alternative to temperature sampling, known as nucleus sampling.
	TopP *float32 `json:"top_p,omitempty"`
	// Stream determines whether to send back a single response or a stream of events.
	Stream bool `json:"stream,omitempty"`
}

// FailoverInfo provides details about an automatic model failover event.
type FailoverInfo struct {
	OriginalModel string `json:"original_model"`
	NewModel      string `json:"new_model"`
	Reason        string `json:"reason"`
}

// GenerationResponse defines the successful response structure sent back to the client.
// It includes the LLM's content plus rich metadata about the generation process.
type GenerationResponse struct {
	// Content is the text generated by the LLM.
	Content string `json:"content"`
	// ModelUsed is the ID of the model that was ultimately selected by the router to process the request.
	ModelUsed string `json:"model_used"`
	// Usage provides token metrics for the entire request, including all tool-use rounds.
	Usage Usage `json:"usage"`
	// LatencyMS is the total end-to-end processing time for the request in milliseconds.
	LatencyMS int64 `json:"latency_ms"`
	// RAGContextUsed indicates whether context from the RAG system was used to augment the prompt.
	RAGContextUsed bool `json:"rag_context_used"`
	// ToolCalls provides a log of any tools that were executed by the agent during the request.
	ToolCalls []ExecutedToolCall `json:"tool_calls,omitempty"`
	// CacheStatus indicates whether the response was served from the cache ("HIT") or generated live ("MISS").
	CacheStatus string `json:"cache_status"`
	// --- ADD THIS LINE ---
	// FailoverInfo will be populated if a session failover occurred during the request.
	FailoverInfo   *FailoverInfo `json:"failover_info,omitempty"`
}

// ExecutedToolCall provides a transparent record of a tool that was executed by the agent.
type ExecutedToolCall struct {
	Name   string `json:"name"`
	Args   string `json:"args"`
	Result string `json:"result"`
}

// Usage mirrors the token usage structure from providers like OpenAI and Anthropic.
type Usage struct {
	PromptTokens     int `json:"prompt_tokens"`
	CompletionTokens int `json:"completion_tokens"`
	TotalTokens      int `json:"total_tokens"`
}

// Add accumulates the token usage from another Usage struct into this one.
// This is essential for correctly tracking total tokens in multi-step agentic chains.
func (u *Usage) Add(other Usage) {
	u.PromptTokens += other.PromptTokens
	u.CompletionTokens += other.CompletionTokens
	u.TotalTokens += other.TotalTokens
}